# 2023.10.6

- 测试对象：karate
- 超参数：
  -  batch size = 128
  -  learning rate = 1e-3
  -  MAX_MEMORY_CAPACITY = 5000
  -  TAU = 0.005
  -  GAMMA = 0.9
  -  EPS_START = 0.9
  - EPS_END = 0.05
  - EPS_DECAY = 1000
- 初始 rumor node 数量/比例：10%
- authoritative T node 数量/比例：1 个
- 奖励函数：
    $$
    \text{reward} = e^{\frac{\text{new}|T|}{\text{last}|T|}-1}-(1.2\times|R|+e^{-1})
    $$
    
    思路：当 T nodes 的增长倍率超过 2 时，才有可能得到正的奖励，否则奖励均为负值


训练结果：

![](result_img/test_karate_rewardFunc-v1.jpg)

一共跑了 20000次 episode；loss 很大；平均 R active nodes 稳定在 8.45 附近；实际上在 8000 次 episode 就已经比较稳定。

实验了一下，效果很差. 跟训练结果接近，或者都没达到（😭😭）

参数：au_T_rate=0.06; r_rate=0.1; T_rate_min=0.1; T_rate_max=0.25; iters=500

|         | 0.1   | 0.11666667 | 0.13333333 | 0.15   | 0.16666667 | 0.18333333 | 0.2    | 0.21666667 | 0.23333333 | 0.25   |
| ------- | ----- | ---------- | ---------- | ------ | ---------- | ---------- | ------ | ---------- | ---------- | ------ |
| R nodes | 9.346 | 9.76       | 9.542      | 9.218  | 8.96       | 9.85       | 9.87   | 9.566      | 9.246      | 9.408  |
| T nodes | 11.84 | 11.862     | 12.594     | 13.592 | 13.634     | 13.548     | 13.236 | 13.132     | 13.838     | 13.844 |

想要超过 `contrid` 算法，至少 $\frac{T}{R}\ge 1.71$; 超过 `M3T` 算法，至少 $\frac{T}{R}\ge 2.39$


**改进思路：** 
1. 修改奖励函数，提高 T node 的奖励
2. 修改 env，在每次 step 之后，可以选择新变成 R-active node 的点作为 action


# 2023.10.7

- 测试对象：karate
- 超参数：
  -  batch size = 128
  -  learning rate = 1e-3
  -  MAX_MEMORY_CAPACITY = 5000
  -  TAU = 0.005
  -  GAMMA = 0.9
  -  EPS_START = 0.9
  - EPS_END = 0.05
  - EPS_DECAY = 1000
- 初始 rumor node 数量/比例：10%
- authoritative T node 数量/比例：1 个
- 奖励函数：
  $$
  \begin{cases}
  \ln(\frac{T}{R}), &\frac{T}{R}<1\\
  e^{\frac{T}{R}}, &else
  \end{cases}
  $$

  思路：放大 $\frac{T}{R}$ 说带来的收益/惩罚


训练结果：

![](result_img/test_karate_rewardFunc-v2.jpg)

经过 80000 次 episode 后，$\frac{T}{R} \approx 4.804$；loss 依旧很大。

感觉还不错？可惜是错觉，基本没什么大的改变。😭🤡

|         | 0.1    | 0.11666667 | 0.13333333 | 0.15   | 0.16666667 | 0.18333333 | 0.2   | 0.21666667 | 0.23333333 | 0.25   |
| ------- | ------ | ---------- | ---------- | ------ | ---------- | ---------- | ----- | ---------- | ---------- | ------ |
| R nodes | 9.946  | 9.23       | 9.822      | 9.432  | 10.17      | 9.532      | 10.17 | 9.264      | 9.436      | 8.79   |
| T nodes | 11.432 | 12.198     | 12.132     | 12.624 | 12.642     | 13.262     | 12.79 | 13.608     | 13.578     | 14.262 |


这结果倒是和之前某次的训练结果类似

![](result_img/karate_test_1.jpg)

R active nodes 的数量在 10 个左右徘徊。


**改进思路：**
1. 对 reward 进行裁剪，因为 loss 依旧很巨大
2. 探索新的 reward 函数
3. 或许 experience pool 太小了？增大试试
4. 巩固一下理论吧


# 2023.10.8

**延迟奖励：**
> 没有标签来说明现在这个 action 是正确还是错误的，必须等到游戏结束才可能知道，这个游戏可能 10s 后才结束。现在这个动作到底对最后游戏是否能赢有无帮助，我们其实是不清楚的。这里我们就面临延迟奖励（delayed reward）的问题，延迟奖励使得训练网络非常困难。

强化学习特征：
> 1. 强化学习会试错探索，它通过探索环境来获取对环境的理解。
> 2. 强化学习智能体会从环境里面获得延迟的奖励。
> 3. 在强化学习的训练过程中，**时间非常重要**。因为我们得到的是有时间关联的数据（sequential data）， 而不是独立同分布的数据。在机器学习中，如果观测数据有非常强的关联，会使得训练非常不稳定。这也是为什么在监督学习中，我们希望数据尽量满足独立同分布，这样就可以消除数据之间的相关性。
> 4. 智能体的动作会影响它随后得到的数据，这一点是非常重要的。在训练智能体的过程中，很多时 候我们也是通过正在学习的智能体与环境交互来得到数据的。所以如果在训练过程中，智能体不能保持稳定，就会使我们采集到的数据非常糟糕。我们通过数据来训练智能体，如果数据有问题，整个训练过程就会失败。所以在强化学习里面一个非常重要的问题就是，**怎么让智能体的动作一直稳定地提升**。



**rollout（预演）**
> 预演是指从当前环境对动作进行采样，生成很多局游戏。将当前的智能体与环境交互，会得到一系列观测。每一个观测可看成一个**轨迹(trajectory)**，即 state 和 action 的序列：
> $$\tau=(s_0,a_0,s_1,a_1,...)$$
>
> 我们可以通过观测序列以及最终奖励(eventual reward)来训练 agent，使它尽可能的采取可以获得最终奖励的动作。



**新想法**：
> 目前的 action 似乎有点多，且表示的意义比较模糊。现在减少 action 的个数，比如在某个 state S1，采用选取当前度最大的点作为action；进入到下一个 state S2, 采用 M3T 策略等。似乎有搞头？😈


# 2023.10.9
根据昨天的想法，policy gradient 的方法应该更适合，比如选用 PPO。不过先试试 DQN 看看效果如何。

更新了 env。具体配置如下：

- 测试对象：karate
- 超参数：
  -  batch size = 128
  -  learning rate = 1e-3
  -  MAX_MEMORY_CAPACITY = 25000
  -  TAU = 0.005
  -  GAMMA = 0.9
  -  EPS_START = 0.9
  - EPS_END = 0.05
  - EPS_DECAY = 1000
- 初始 rumor node 数量/比例：10%
- authoritative T node 数量/比例：1 个
- select T nodes 的数量: 3 个
- 奖励函数：
    $$
    \text{reward} =
    \begin{cases}
    |T|-|R|, &\text{terminated or truncated}\\
    0, &\text{else}
    \end{cases}
    $$
    思路：朴素无华，延迟奖励，只有结束时才有 reward，否则为 0

本次重点对环境进行了更新，将 action 减少为 3 个，具体如下:
```python
self.action_space = spaces.Discrete(3)
self._action_to_algorithm = {
    0:self._action_degree_based,
    1:self._action_M3T_based,
    2:self._action_random_based
}
```
3 个 action 分别对应了 `degree`,`M3T`,`random` 三种算法

训练结果图如下：

![](result_img/test_karate_newAction-v1_k=3.jpg)

- loss 稳定下降，并且还有下降空间
- 前 10000 次 episode 得分变化不大，之后有一个明显的上生趋势

average score 从 3.10145 升至 3.18 后左右波动了很长一段时间，之后稳定上升至 3.7723

实际试验一下：

|         | 0.1   | 0.11666667 | 0.13333333 | 0.15   | 0.16666667 | 0.18333333 | 0.2    | 0.21666667 | 0.23333333 | 0.25   |
| ------- | ----- | ---------- | ---------- | ------ | ---------- | ---------- | ------ | ---------- | ---------- | ------ |
| R nodes | 7.56  | 7.7        | 7.164      | 7.272  | 7.434      | 6.576      | 6.9    | 6.67       | 6.622      | 6.082  |
| T nodes | 19.51 | 19.238     | 20.672     | 21.552 | 21.498     | 22.958     | 22.916 | 23.676     | 23.44      | 24.436 |

效果好多了 😁👍

`contrid` 的结果：
|         | 0.1   | 0.11666667 | 0.13333333 | 0.15   | 0.16666667 | 0.18333333 | 0.2    | 0.21666667 | 0.23333333 | 0.25   |
| ------- | ----- | ---------- | ---------- | ------ | ---------- | ---------- | ------ | ---------- | ---------- | ------ |
| R nodes | 8.162 | 8.304      | 7.662      | 7.29   | 7.636      | 7.156      | 7.336  | 7.592      | 7.332      | 7.056  |
| T nodes | 14.4  | 14.548     | 15.954     | 17.354 | 16.644     | 17.846     | 17.884 | 18.552     | 18.422     | 19.506 |

`M3T` 的结果：
|         | 0.1    | 0.11666667 | 0.13333333 | 0.15   | 0.16666667 | 0.18333333 | 0.2    | 0.21666667 | 0.23333333 | 0.25  |
| ------- | ------ | ---------- | ---------- | ------ | ---------- | ---------- | ------ | ---------- | ---------- | ----- |
| R nodes | 6.988  | 7.026      | 6.432      | 5.79   | 5.572      | 5.69       | 5.482  | 5.236      | 5.458      | 5.394 |
| T nodes | 18.008 | 17.506     | 19.906     | 22.518 | 22.698     | 24.228     | 24.306 | 25.716     | 25.688     | 26.6  |

效果已经是超过 `contrid` ，但与 `M3T` 还是有点差距


# 2023.10.10

- 测试对象：karate
- 超参数：
  -  batch size = 128
  -  learning rate = 1e-3
  -  MAX_MEMORY_CAPACITY = 25000
  -  TAU = 0.005
  -  GAMMA = 0.9
  -  EPS_START = 0.9
  - EPS_END = 0.05
  - EPS_DECAY = 1000
- 初始 rumor node 数量/比例：10%
- authoritative T node 数量/比例：1 个
- select T nodes 的数量: 10 个
- 奖励函数：
    $$
    \text{reward} =
    \begin{cases}
    |T|-|R|, &\text{terminated or truncated}\\
    0, &\text{else}
    \end{cases}
    $$

现在将 select T nodes 的数量提升到 10 个，接近karate网络 $\frac{1}{3}$ 的节点数。

训练结果

![](result_img/test_karate_newAction-v1_k=10.jpg)

- 20000 次 episode 的 average score 为 5.7983, 有缓慢上升的趋势；loss 为 3.4507， 也有逐渐减小的趋势。
- 总体而言没什么大的变化趋势

测试效果：
|         | 0.1    | 0.11666667 | 0.13333333 | 0.15   | 0.16666667 | 0.18333333 | 0.2    | 0.21666667 | 0.23333333 | 0.25   |
| ------- | ------ | ---------- | ---------- | ------ | ---------- | ---------- | ------ | ---------- | ---------- | ------ |
| R nodes | 9.42   | 9.436      | 8.522      | 7.97   | 7.924      | 7.432      | 7.108  | 6.666      | 7.096      | 6.3    |
| T nodes | 13.554 | 13.068     | 15.324     | 18.122 | 18.326     | 20.142     | 20.442 | 22.158     | 21.658     | 23.138 |

实际效果更差了。

`degree` 的结果：
|         | 0.1    | 0.11666667 | 0.13333333 | 0.15   | 0.16666667 | 0.18333333 | 0.2    | 0.21666667 | 0.23333333 | 0.25   |
| ------- | ------ | ---------- | ---------- | ------ | ---------- | ---------- | ------ | ---------- | ---------- | ------ |
| R nodes | 7.746  | 7.9        | 7.07       | 7.348  | 7.09       | 6.782      | 6.692  | 6.83       | 6.578      | 6.564  |
| T nodes | 19.626 | 19.436     | 21.474     | 22.032 | 22.476     | 23.352     | 23.254 | 23.882     | 24.082     | 24.672 |

实际上之前的 `k=3` 的网络最终的选择效果，与 `degree` 是接近的。

**问题：** 
1. 通过dqn网络进行选择，和随机选择这 3 个 algorithm 来选取 node，是否有区别？
2. 更改 `select_k_Tnodes` 这个参数，是否对训练结果有影响？是否存在一个最优的 `k` 值？


# 2023.10.11

测试对象同 `2023.10.10`.

**探究问题**：不同 k 值是否会影响测试结果
- `select_k_Tnodes=1`: 在接近 20000 次 episode时，得分出现下降，loss 也大幅提升；之后 20000~40000 次 episode 时，得分逐渐上涨，稳定在 1.79 附近。
  ![](result_img/test_karate_newAction-v1_k=1.jpg)
- `select_k_Tnodes=3`: 之前训练的结果图,得分 3.77 附近。
  ![](result_img/test_karate_newAction-v1_k=3.jpg)
- `select_k_Tnodes=5`:实际上20000多次后，得分就在 4.87 附近小幅变化。
  ![](result_img/test_karate_newAction-v1_k=5.jpg)
- `select_k_Tnodes=7`:比较震荡，得分经常下降后又快速上升。接近 20000 次 episode 时，得分在 5.32 附近。
  ![](result_img/test_karate_newAction-v1_k=7.jpg)


实验测试
- k=1

|         | 0.1    | 0.11666667 | 0.13333333 | 0.15   | 0.16666667 | 0.18333333 | 0.2    | 0.21666667 | 0.23333333 | 0.25   |
| ------- | ------ | ---------- | ---------- | ------ | ---------- | ---------- | ------ | ---------- | ---------- | ------ |
| R nodes | 7.324  | 8.306      | 6.966      | 6.922  | 7.112      | 6.518      | 6.934  | 6.55       | 7.178      | 6.72   |
| T nodes | 19.168 | 18.352     | 20.822     | 22.124 | 21.694     | 23.37      | 22.718 | 23.804     | 23.43      | 24.456 |

- k=3

|         | 0.1   | 0.11666667 | 0.13333333 | 0.15   | 0.16666667 | 0.18333333 | 0.2    | 0.21666667 | 0.23333333 | 0.25   |
| ------- | ----- | ---------- | ---------- | ------ | ---------- | ---------- | ------ | ---------- | ---------- | ------ |
| R nodes | 7.56  | 7.7        | 7.164      | 7.272  | 7.434      | 6.576      | 6.9    | 6.67       | 6.622      | 6.082  |
| T nodes | 19.51 | 19.238     | 20.672     | 21.552 | 21.498     | 22.958     | 22.916 | 23.676     | 23.44      | 24.436 |

- k=5

|         | 0.1    | 0.11666667 | 0.13333333 | 0.15   | 0.16666667 | 0.18333333 | 0.2    | 0.21666667 | 0.23333333 | 0.25   |
| ------- | ------ | ---------- | ---------- | ------ | ---------- | ---------- | ------ | ---------- | ---------- | ------ |
| R nodes | 7.664  | 8.1        | 7.504      | 7.576  | 7.614      | 7.232      | 6.742  | 6.644      | 6.586      | 6.138  |
| T nodes | 19.116 | 18.6       | 20.39      | 20.932 | 20.696     | 21.978     | 22.474 | 23.03      | 22.954     | 23.978 |

- k=7

|         | 0.1    | 0.11666667 | 0.13333333 | 0.15  | 0.16666667 | 0.18333333 | 0.2    | 0.21666667 | 0.23333333 | 0.25  |
| ------- | ------ | ---------- | ---------- | ----- | ---------- | ---------- | ------ | ---------- | ---------- | ----- |
| R nodes | 7.89   | 7.814      | 7.426      | 7.11  | 6.842      | 6.988      | 6.73   | 6.888      | 6.656      | 6.63  |
| T nodes | 18.964 | 19.206     | 20.806     | 22.07 | 21.95      | 22.748     | 22.816 | 23.29      | 23.856     | 24.47 |

- k=10

|         | 0.1    | 0.11666667 | 0.13333333 | 0.15   | 0.16666667 | 0.18333333 | 0.2    | 0.21666667 | 0.23333333 | 0.25   |
| ------- | ------ | ---------- | ---------- | ------ | ---------- | ---------- | ------ | ---------- | ---------- | ------ |
| R nodes | 9.42   | 9.436      | 8.522      | 7.97   | 7.924      | 7.432      | 7.108  | 6.666      | 7.096      | 6.3    |
| T nodes | 13.554 | 13.068     | 15.324     | 18.122 | 18.326     | 20.142     | 20.442 | 22.158     | 21.658     | 23.138 |


取不同 `select_k_Tnodes` 的实验结果对比图：

![](result_img/test_karate_newAction-v1_differenet_k.jpg)

- 当 `k=10` 时，效果是明显不好的；
- 其他取值的实验结果差距不是很显著，但直觉告诉我，似乎 `k=3` 时，综合效果最好。


**探究问题**：dqn 网络的选择与随机选择是否有区别？

实验结果
- 3 种算法随机选择

|         | 0.1   | 0.11666667 | 0.13333333 | 0.15   | 0.16666667 | 0.18333333 | 0.2   | 0.21666667 | 0.23333333 | 0.25   |
| ------- | ----- | ---------- | ---------- | ------ | ---------- | ---------- | ----- | ---------- | ---------- | ------ |
| R nodes | 9.272 | 9.076      | 9.494      | 9.476  | 9.302      | 9.134      | 8.912 | 9.19       | 9.318      | 9.312  |
| T nodes | 12.32 | 12.4       | 12.756     | 12.836 | 12.728     | 12.564     | 13.06 | 13.126     | 12.66      | 13.234 |

破案了，有差别，而且还很大。


**思考**：
1. 目前训练的 dqn 似乎更倾向于选择 `degree` 这个算法，是否是因为其他 2 个算法相对而言获得的奖励不是很大？
2. 考虑采用 `PPO` 来替换 `DQN`

# 2023.10.15

使用了 `PPO` 来训练模型，训练结果和 `DQN` 是差不多的。

**思考**：
1. 是否是 action 中 `random` 拉低了性能？
2. 原始版本的 `LTTD` environment 可选择的动作为图 G 的顶点数，此时就会有一个 `invalid action` 的问题，即不能选择状态为 `R` or `T` 的节点。
3. environment 的逻辑可能需要优化改善。


**对原始版本的 `LTTD` environment (LTTD-v0) 进行了改进**：
1. 同样是选取一个 action 后评估奖励，该 reward 为该 action (node) 对其邻居节点的影响 $\Delta T - \Delta R$, 但不进行传播；
2. 当选取最后一个 action，即第 `select_k_Tnodes` 后，进行传播。传播结束后的 reward 为 $\frac{|T|-|R|}{\text{select k T nodes}}$, 因为最后的 reward 是该 episode 中所有 action 共同贡献的结果，所以取平均值。**不排除后续有更好的计算方法**。



# 2023.10.16

在更新 `LTTD-v0` 后，对原始版本的 dqn 中 `invalid action` 采取屏蔽操作。

测试结果如下：

- `k=3`

|         | 0.1   | 0.11666667 | 0.13333333 | 0.15   | 0.16666667 | 0.18333333 | 0.2    | 0.21666667 | 0.23333333 | 0.25   |
| ------- | ----- | ---------- | ---------- | ------ | ---------- | ---------- | ------ | ---------- | ---------- | ------ |
| R nodes | 9.932 | 10.294     | 9.068      | 8.412  | 8.698      | 7.778      | 8.112  | 8.478      | 8.114      | 8.132  |
| T nodes | 9.618 | 9.642      | 12.39      | 13.864 | 13.468     | 15.712     | 15.924 | 16.364     | 16.648     | 17.892 |

- `k=5`

|         | 0.1   | 0.11666667 | 0.13333333 | 0.15   | 0.16666667 | 0.18333333 | 0.2    | 0.21666667 | 0.23333333 | 0.25   |
| ------- | ----- | ---------- | ---------- | ------ | ---------- | ---------- | ------ | ---------- | ---------- | ------ |
| R nodes | 7.716 | 7.684      | 8.196      | 7.32   | 7.626      | 7.674      | 8.09   | 7.662      | 7.82       | 7.294  |
| T nodes | 14.39 | 13.976     | 14.8       | 16.416 | 16.17      | 16.012     | 16.104 | 15.824     | 15.842     | 16.462 |


效果比最初好了那么一点点，但依旧很差。

训练结果和之前差不多，loss 依旧稳定上升，得分小幅震荡上升


# 2023.10.17

- 根据昨天修改 `LTTD-v0` 的 reward 的思路，修改一下 `LTTD-v1` 的 reward。
  省流：没啥显著变化
- 将 `LTTD-v1` 的 `random` 这个 action 去除：
  再次省流：没啥显著变化，依旧接近 degree 的算法。

修改 `LTTD-v1` reward 计算方式，提高纠正 R node 的得分:

计算 $\Delta T$ 和 $\Delta R$, 如果 $\Delta R <0$, 则 $\Delta R = \alpha \times \Delta R$, $\alpha>0$.

$$
reward = \begin{cases}
  \frac{\Delta T - \Delta R}{\text{budget k}}, & \Delta R >0 \\
  \frac{\Delta T - \alpha\times\Delta R}{\text{budget k}}, & \Delta R <0 
\end{cases}
$$

训练时 k=3.
- $\alpha=1.5$. 省流：和之前差不多。
- $\alpha=3$. 同上。


训练时 k=8.
- $\alpha=1.5$. 省流：和之前差不多。
- $\alpha=3$. 同上。


**思考**：
1. 是否是 karate club 的规模太小？考虑对多个数据集进行测试。
2. 目前可以确定的一个现象是，训练时，*k 的取值不用太大*。考虑在网络中加入 `dropout` 层。



# 2023.10.18

加上 `Dropout` 层：
```
Net_Karate(
  (flatten): Flatten(start_dim=1, end_dim=-1)
  (lin_relu): Sequential(
    (0): Linear(in_features=34, out_features=64, bias=True)
    (1): ReLU()
    (2): Dropout(p=0.2, inplace=False)
    (3): Linear(in_features=64, out_features=32, bias=True)
    (4): ReLU()
    (5): Linear(in_features=32, out_features=2, bias=True)
  )
)
```
对于 `LTTD-v1`

- 修改奖励函数：

  当选取某个 action(node) 时，预估其周围邻居节点的状况，当可能变为 `T-active` 时，$\Delta T+1$;
  当可能将 `R-active` 转换为 `T-active` 时，$\Delta R-1$.

  如果 action 的邻居没有 `R-active` nodes，则 $\Delta T = 0.5\Delta T$;
  若有，则 $\Delta R = \text{num-of-R-nbr}\times (\Delta R -1)$.

  经过修正后，
  $$
  reward = \Delta T - \Delta R
  $$

- 将 action 中的 `random` 替换为 `contrid`

测试结果：无显著变化，换成 `contrid` 后效果进一步下降。🤡😭


对于 `LTTD-v0`：

- 修改奖励函数：
  $$
  reward = \begin{cases}
  1, &\text{spread done and } T\ge R \\
  -1, &\text{spread done and } T< R \\
  0, &\text{spreading}
  \end{cases}
  $$
- target net 每 100 次 episode 后进行更新，而不是每次 episode 后进行微调
- batch size 增大至 256
- 超参数：
  - GAMMA = 0.6
  - EPS_START = 0.9
  - EPS_END = 0.05
  - EPS_DECAY = 10000

效果：
- loss 可以稳定下降了
- 当 k 较小时，经过 100,000 次训练，效果依旧非常差。
- 当 k=8 时，训练效果好了那么一丁点. loss 稳定下降，得分上升，但很慢。
  ![](result_img/test_karate_v0_k=8.jpg)
  测试结果：很一般，不是很理想。

  |         | 0.1   | 0.11666667 | 0.13333333 | 0.15   | 0.16666667 | 0.18333333 | 0.2    | 0.21666667 | 0.23333333 | 0.25  |
  | ------- | ----- | ---------- | ---------- | ------ | ---------- | ---------- | ------ | ---------- | ---------- | ----- |
  | R nodes | 9.952 | 10.714     | 9.688      | 8.346  | 9.008      | 8.382      | 8.23   | 8.098      | 8.212      | 7.038 |
  | T nodes | 9.526 | 9.75       | 12.104     | 14.694 | 14.6       | 16.746     | 17.488 | 18.668     | 18.488     | 19.84 |

- k=5. 将学习率调整为 $10^{-3}$

  测试结果：效果又好起来了?

  |         | 0.1    | 0.11666667 | 0.13333333 | 0.15   | 0.16666667 | 0.18333333 | 0.2    | 0.21666667 | 0.23333333 | 0.25   |
  | ------- | ------ | ---------- | ---------- | ------ | ---------- | ---------- | ------ | ---------- | ---------- | ------ |
  | R nodes | 8.416  | 7.918      | 7.684      | 6.758  | 7.192      | 7.186      | 6.82   | 6.902      | 6.602      | 6.76   |
  | T nodes | 16.986 | 18.16      | 19.532     | 20.096 | 20.688     | 20.992     | 22.222 | 22.224     | 23.208     | 23.204 |

- k=3. 学习率等同于 k=5. 80,000 episodes 耗时 40m。
  测试结果：居然接近 `LTTD-v1` 的效果！

  |         | 0.1    | 0.11666667 | 0.13333333 | 0.15   | 0.16666667 | 0.18333333 | 0.2    | 0.21666667 | 0.23333333 | 0.25   |
  | ------- | ------ | ---------- | ---------- | ------ | ---------- | ---------- | ------ | ---------- | ---------- | ------ |
  | R nodes | 7.29   | 7.54       | 7.274      | 6.964  | 6.492      | 6.728      | 6.766  | 6.442      | 6.366      | 6.416  |
  | T nodes | 19.826 | 20.776     | 21.976     | 22.352 | 23.132     | 23.218     | 23.454 | 23.864     | 24.284     | 24.326 |

**近期实验总结：**
对于 karate 网络
1. 总体而言，k 值不需要太大。
2. `LTTD-v0` 的训练时长比较长，loss 不一定能稳定下降，但最近几次都能稳定下降，且效果有提升，十分接近 `degree` 算法。
3. `LTTD-v1` 的训练时长相对快一些，loss 可以稳定下降，测试效果通常持平 `degree` 算法。


# 2023.10.21

- 目标网络 dolphins
- dqn 结构
  ```
      Net_Dolphin(
      (lin_relu): Sequential(
        (0): Linear(in_features=62, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
        (3): ReLU()
        (4): Linear(in_features=64, out_features=62, bias=True)
      )
    )
  ```
- 超参数：
  - BATCH_SIZE_DOL = 256
  - LEARNING_RATE_DOL = 1e-3
  - MAX_MEMORY_CAPACITY = 30000
  - GATHER_EXPERIENCE_SIZE = 100
  - TARGET_REPLACE_ITER = 200
  - TAU = 0.005
  - GAMMA = 0.6
  - EPS_START = 0.9
  - EPS_END = 0.05
  - EPS_DECAY = 10000


> k = 8
训练结果：

![](result_img/dolphins_v0_k=8.jpg)

测试效果：

|         | 0.1    | 0.11666667 | 0.13333333 | 0.15   | 0.16666667 | 0.18333333 | 0.2    | 0.21666667 | 0.23333333 | 0.25   |
| ------- | ------ | ---------- | ---------- | ------ | ---------- | ---------- | ------ | ---------- | ---------- | ------ |
| R nodes | 13.686 | 13.27      | 12.51      | 12.338 | 11.842     | 11.462     | 11.692 | 11.308     | 11.06      | 10.844 |
| T nodes | 25.298 | 27.748     | 30.788     | 32.944 | 34.148     | 35.422     | 37.06  | 38.464     | 39.168     | 41.366 |

传统算法：
final R nodes
|         | 0.1    | 0.11666667 | 0.13333333 | 0.15   | 0.16666667 | 0.18333333 | 0.2    | 0.21666667 | 0.23333333 | 0.25   |
| ------- | ------ | ---------- | ---------- | ------ | ---------- | ---------- | ------ | ---------- | ---------- | ------ |
| Degree  | 12.938 | 12.318     | 12.372     | 11.716 | 11.616     | 11.314     | 10.876 | 11.004     | 10.844     | 10.576 |
| ContrId | 12.576 | 11.988     | 11.61      | 11.361 | 11.306     | 10.996     | 10.706 | 10.562     | 10.364     | 10.21  |
| TCS     | 16.338 | 16.516     | 15.632     | 14.852 | 15.08      | 14.292     | 14.436 | 13.498     | 13.33      | 13.006 |
| Greedy  | 9.65   | 9.588      | 8.624      | 8.822  | 8.644      | 8.284      | 8.856  | 8.358      | 8.754      | 8.506  |
| M3T     | 12.504 | 12.15      | 11.604     | 11.131 | 10.444     | 10.072     | 9.791  | 9.61       | 9.462      | 9.422  |

对比图：

![](result_img/cmp_dolphin_v0.jpg)

总体处于中间水平。

# 2023.10.22

奖励函数是有影响的，之前写的测试函数可能有问题，导致结果很差(恼😡)

调整后的结果：

|         | 0.1    | 0.11666667 | 0.13333333 | 0.15   | 0.16666667 | 0.18333333 | 0.2    | 0.21666667 | 0.23333333 | 0.25   |
| ------- | ------ | ---------- | ---------- | ------ | ---------- | ---------- | ------ | ---------- | ---------- | ------ |
| R nodes | 13.364 | 12.526     | 12.182     | 11.578 | 11.604     | 11.274     | 11.256 | 11.132     | 10.82      | 10.494 |
| T nodes | 26.706 | 28.336     | 31.55      | 33.392 | 34.976     | 36.16      | 37.6   | 39.168     | 40.408     | 42.744 |

好吧，有，但不大。

# 2023.10.26

经过 GAE 得到 embeddings 后再输入 DQN。省流：在 karate 的测试中，最终效果变化不大。

# 2023.10.27

新思路：
1. 根据 GraphSage 的思想，聚合 inactive 节点周围 T,R 节点的信息，赋值给 inactive 节点，根据值对节点进行选取。
2. 训练 DQN 只用于寻找能够使得 T 影响力最大的节点。


# 2023.10.29

实验对象：food
参数设置：
- BATCH_SIZE_FB = 64
- LEARNING_RATE_FB = 1e-4
- MAX_MEMORY_CAPACITY = 100,000
- GATHER_EXPERIENCE_SIZE = 100
- TARGET_REPLACE_ITER = 200
- TAU = 0.005
- EPS_DECAY = 30000
- episode = 100,000
- k=20

训练结果：

![](result_img/food_v0_k=20.jpg)

测试结果

![](result_img/cmp_food_v0.jpg)

具体数据：

|         | 0.01    | 0.014   | 0.018   | 0.023   | 0.0278  | 0.032   | 0.036   | 0.041   | 0.045   | 0.05    |
| ------- | ------- | ------- | ------- | ------- | ------- | ------- | ------- | ------- | ------- | ------- |
| R nodes | 144.708 | 141.114 | 140.486 | 137.444 | 125.302 | 128.31  | 125.342 | 120.09  | 117.238 | 115.868 |
| T nodes | 79.738  | 106.49  | 133.358 | 145.378 | 162.998 | 172.838 | 185.34  | 195.158 | 201.244 | 212.552 |

DQN 似乎还是偏向去扩散 T，忽略了 R 的遏制。总体水平是中等。得想办法引导 DQN 去限制 R 的传播。


大致估算了一下，DQN 若要超过其他算法，至少需要：

final_R_recv:
- dolphin: 15% ~ 20%.
- food: 14% ~ 24%.
- netsci: 10% ~ 15%.
- uspower: 11% ~ 16%.

final_T_revc:
- dolphin: 46% ~ 66%.
- food: 19% ~ 38%.
- netsci: 5% ~ 19%.
- uspower: 9% ~ 30%.

如果考虑 R seed 和 budget k:
dolphins: seed R rate 10%, budget k rate 10% ~ 25%.
other: seed R rate 5%, budget k rate 1% ~ 5%

$$
\text{平均值} = \frac{\text{final T or R recv}}{\text{R seed rate}*\text{budget k}}
$$

则
final_R_recv:
- dolphin: 6 ~ 20
- food: 60 ~ 481
- netsci: 43 ~ 307
- uspower: 45 ~ 333

final_T_recv:
- dolphin: 26.4 ~ 46
- food: 152 ~ 380
- netsci: 76 ~ 100
- uspower: 120 ~ 180


# 2023.11.1

或许训练时，将 LTTD 的阈值固定住，即 reset 时不重置这些阈值，是否会更容易点？

# 2023.11.2

修改了 lttd 的 reward_shaping：
- 每次选择时，将大致评估 action 选取后的传播效果，统计 delta_T 和 delta_R 的变化，reward=delta_T-delta_R
- 最后一个 action 时，reward=delta_T-delta_R，当
  - RN_rate <= target_to_reach 时，reward*=1.5.
  - TR_rate >= 5 时，reward*=1.5
  - TR_rate >= 2 时，reward*=1.1

修改了 DQN 的 GAMMA 值，从 0.6 降至 0.55

经过20000次 episode 训练后进行测试：

|         | 0.1    | 0.11666667 | 0.13333333 | 0.15   | 0.16666667 | 0.18333333 | 0.2    | 0.21666667 | 0.23333333 | 0.25   |
| ------- | ------ | ---------- | ---------- | ------ | ---------- | ---------- | ------ | ---------- | ---------- | ------ |
| R nodes | 12.616 | 12.708     | 12.308     | 12.042 | 11.678     | 11.37      | 11.092 | 11.154     | 10.68      | 10.478 |
| T nodes | 29.294 | 30.444     | 32.212     | 34.616 | 35.942     | 37.908     | 38.38  | 39.472     | 40.787     | 42.744 |

好了那么一丢丢, 但没发生质变。

![](result_img/cmp_dolphin_v0_1.jpg)

# 2023.11.4

之前训练时，dolphins 的训练参数设置错误，将 au_T_rate 设置成 0.01，而测试时的 au_T_rate 为 0.08，因此效果有些问题。目前已修正。

使用 vector environment，每个 env 设置不同参数。
使用 A2C 进行训练。

参数设置：

dolphin 网络

环境参数：
- n_envs = 3
- n_updates = 1000 # 对 actor，critic 网络进行更新的次数
- n_steps_per_update = 128 # 在每次更新前进行，需要 play 的次数

```python
envs = gym.vector.AsyncVectorEnv(
    [
        lambda: gym.make(
            'LTTD-v0',
            G=G1,
            init_rumor_rate=0.1,
            au_T_rate=0.08,
            k_budget=0.10,
            target_to_reach=0.155,
        ),
        lambda: gym.make(
            'LTTD-v0',
            G=G1,
            init_rumor_rate=0.1,
            au_T_rate=0.08,
            k_budget=0.15,
            target_to_reach=0.147,
        ),
        lambda: gym.make(
            'LTTD-v0',
            G=G1,
            init_rumor_rate=0.1,
            au_T_rate=0.08,
            k_budget=0.25,
            target_to_reach=0.137,
        ),
    ]
)
```

agent hyperparams：
- gamma = 0.6 # 这个值跟 DQN 中的 gamma 是同一个，太大 loss 不容易收敛
- lam = 0.95  # hyperparameter for GAE
- ent_coef = 0.01  # coefficient for the entropy bonus (to encourage exploration)
- actor_lr = 0.001
- critic_lr = 0.005

训练结果

![](result_img/dolphin_a2c_v0_vec_env3_1.png)

这个 return 看起来不是很明显，但是 loss 都是稳定下降的。

测试结果：

|         | 0.1    | 0.11666667 | 0.13333333 | 0.15   | 0.16666667 | 0.18333333 | 0.2    | 0.21666667 | 0.23333333 | 0.25   |
| ------- | ------ | ---------- | ---------- | ------ | ---------- | ---------- | ------ | ---------- | ---------- | ------ |
| R nodes | 12.874 | 12.746     | 12.008     | 11.958 | 11.428     | 11.488     | 10.898 | 10.994     | 10.46      | 10.362 |
| T nodes | 29.594 | 31.752     | 34.354     | 36.556 | 38.476     | 40.128     | 40.876 | 42.26      | 43.462     | 44.808 |

T nodes 的数量提升很明显。


# 2023.11.12

折腾了好几天，这个 `n_step_update` 需要大一点，比如 128，训练结果会比较稳定。如果太小 `Critic loss` 震荡的很剧烈。

但是训练的就比较慢。

# 2023.11.16

训练 A2C 需要运气，对初始参数十分敏感。不一定能复现之前的效果。

# 2023.11.21

当时在 A2C 中添加了 `initialize()`，画蛇添足，搞得算法效果很差。

# 2023.12.19

又重新看了一下 DQN 的代码，其中 `target_net` 在选择 max Q value 时，没有考虑valid action 的问题，这会不会对训练结果造成影响？

# 2023.12.20

大致实现了 embedding+DQN 的结构。但是在更新 node embedding 时，由于需要 $\sum_{u\in N(v)}x_u^t$ ，多次这样操作后得到的 embedding 将会溢出。

# 2023.12.20

`loss.backward()` 时出现报错 "Trying to backward through the graph a second time...". 原因是代码中的有些操作是 inplace ，比如从某个 tensor 中取出某一维，如果修改了这取出的一维中的元素，就是一个 inplace 操作。

# 2023.12.23

但经过多天排查，代码中并没有 inplace 的操作。通过 [pytorch forum](https://discuss.pytorch.org/t/runtimeerror-one-of-the-variables-needed-for-gradient-computation-has-been-modified-by-an-inplace-operation-code-worked-in-pytorch-1-2-but-not-in-1-5-after-updating/87327) 中的解释，优化器的 `.step()` 对权重的改变是 inplace 的。

这也就解释了为什么第一次 `backward` 是成功的，而第二次 `backward` 就会抛出 "RuntimeError: one of the variables needed for gradient computation has been modified by an inplace operation" 错误。

# 2023.12.26

不按之前的方式了，换个思路，收集完 experience 之后，在 learn 之时再转换为对应的 embedding。

已经训练完 dolphin 这个数据集了。 

参数设置:
- BATCH_SIZE_DOL = 64
- NODE_NUM_DOL = G1.number_of_nodes()
- EPS_DECAY_DOL = 10000
- EPS_START_DOL = 0.99
- EPS_END_DOL = 0.05
- GAMMA_DOL = 0.9
- LEARNING_RATE_DOL = 1e-3
- MAX_MEMORY_CAPACITY = 10000
- embedding_dim = 32



结果如下:



|         | 0.1   | 0.116  | 0.133  | 0.15   | 0.166 | 0.183  | 0.2    | 0.216  | 0.233  | 0.25   |
| ------- | ----- | ------ | ------ | ------ | ----- | ------ | ------ | ------ | ------ | ------ |
| R nodes | 10.886 | 10.712 | 10.396 | 10.056 | 9.452 | 9.354  | 9.188  | 9.132  | 9.006   | 8.668  |
| T nodes | 31.86 | 33.312 | 36.256 | 38.798 | 40.012 | 41.872 | 43.018 | 44.158 | 45.338 | 46.846 |

已经超过 `M3T`，领先于 DQN 和 A2C 了，并且在 R_nodes 数量上基本持平于 greedy 算法 (喜😁)
传播真相的能力仅次于 greedy。